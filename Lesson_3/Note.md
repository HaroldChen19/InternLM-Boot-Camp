# 基于InternLM和LangChain搭建你的知识库
This is my course notes of the 3nd class of InternLM-Tutorial

## 大模型开发范式


- LLM的局限性
    - 知识时效性受限：如何让LLM能够获取最新的知识
    - 专业能力有限：如何打造垂域大模型
    - 定制化成本高：如何打造个人专属的LLM应用

- RAG
    - 低成本
    - 可实时性更新
    - 受基座模型影响大
    - 单次回答知识有限

- Finetune
    - 可个性化微调
    - 知识覆盖面广
    - 成本高昂
    - 无法实时更新


## LangChain简介

- 核心组成模块
    - Chains: 将组件组合实现端到端应用，通过一个对象封装实现一系列LLM操作；
    - Eg. 检索问答链，覆盖实现了RAG的全部流程。


## 构建向量数据库

- 确定源文件类型，针对不同类型源文件选用不同的加载器
    - 核心在于将带格式文本转化为无格式字符串

- 由于单个文档往往超过模型上下文上限，我们需要对加载的文档进行切分
    - 一般按字符串长度进行分割
    - 可以手动控制分割块的长度和重叠区间长度

- 使用向量数据库来支持语义检索，需要将文档向量化存入向量数据库
    - 可以使用任意一种Embedding模型来进行向量化
    - 可以使用多种支持语义检索的向量数据库，一般使用轻量级的Chroma


## 搭建知识库助手

- 基于RAG的问答系统性能核心受限于：
    - 检索精度
    - Prompt性能

- 一些可能的优化点
    - 检索方面
        - 基于语义进行分割，保证每一个chunk的语义完整
        - 给每一个chunk生成概括性索引，检索时匹配索引
    - prompt
        - 迭代优化prompt策略
